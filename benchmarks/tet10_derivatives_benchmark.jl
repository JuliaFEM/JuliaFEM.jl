# Benchmark: Tet10 Shape Function Derivatives - Manual vs AD
# 
# Compares two approaches:
# 1. Manual: Hand-calculated derivatives (traditional FEM)
# 2. AD: Automatic differentiation using Tensors.jl gradient()
#
# Run with: julia --project=. benchmarks/tet10_derivatives_benchmark.jl

using BenchmarkTools
using Tensors
using Printf

println("="^70)
println("Tet10 Shape Function Derivatives: Manual vs AD Benchmark")
println("="^70)
println()

# ============================================================================
# METHOD 1: MANUAL (Hand-Calculated Derivatives)
# ============================================================================

"""
Tet10 shape functions (manual implementation).
Reference element: Œæ ‚àà [0,1], Œ∑ ‚àà [0,1], Œ∂ ‚àà [0,1], Œæ+Œ∑+Œ∂ ‚â§ 1
"""
module ManualTet10
using Tensors

# Shape functions
@inline N1(Œæ, Œ∑, Œ∂) = (1 - Œæ - Œ∑ - Œ∂) * (2 * (1 - Œæ - Œ∑ - Œ∂) - 1)
@inline N2(Œæ, Œ∑, Œ∂) = Œæ * (2 * Œæ - 1)
@inline N3(Œæ, Œ∑, Œ∂) = Œ∑ * (2 * Œ∑ - 1)
@inline N4(Œæ, Œ∑, Œ∂) = Œ∂ * (2 * Œ∂ - 1)
@inline N5(Œæ, Œ∑, Œ∂) = 4 * Œæ * (1 - Œæ - Œ∑ - Œ∂)
@inline N6(Œæ, Œ∑, Œ∂) = 4 * Œæ * Œ∑
@inline N7(Œæ, Œ∑, Œ∂) = 4 * Œ∑ * (1 - Œæ - Œ∑ - Œ∂)
@inline N8(Œæ, Œ∑, Œ∂) = 4 * Œ∂ * (1 - Œæ - Œ∑ - Œ∂)
@inline N9(Œæ, Œ∑, Œ∂) = 4 * Œæ * Œ∂
@inline N10(Œæ, Œ∑, Œ∂) = 4 * Œ∑ * Œ∂

# Derivatives (calculated by hand - error-prone!)
@inline dN1_dŒæ(Œæ, Œ∑, Œ∂) = 4 * Œæ + 4 * Œ∑ + 4 * Œ∂ - 3
@inline dN1_dŒ∑(Œæ, Œ∑, Œ∂) = 4 * Œæ + 4 * Œ∑ + 4 * Œ∂ - 3
@inline dN1_dŒ∂(Œæ, Œ∑, Œ∂) = 4 * Œæ + 4 * Œ∑ + 4 * Œ∂ - 3

@inline dN2_dŒæ(Œæ, Œ∑, Œ∂) = 4 * Œæ - 1
@inline dN2_dŒ∑(Œæ, Œ∑, Œ∂) = 0.0
@inline dN2_dŒ∂(Œæ, Œ∑, Œ∂) = 0.0

@inline dN3_dŒæ(Œæ, Œ∑, Œ∂) = 0.0
@inline dN3_dŒ∑(Œæ, Œ∑, Œ∂) = 4 * Œ∑ - 1
@inline dN3_dŒ∂(Œæ, Œ∑, Œ∂) = 0.0

@inline dN4_dŒæ(Œæ, Œ∑, Œ∂) = 0.0
@inline dN4_dŒ∑(Œæ, Œ∑, Œ∂) = 0.0
@inline dN4_dŒ∂(Œæ, Œ∑, Œ∂) = 4 * Œ∂ - 1

@inline dN5_dŒæ(Œæ, Œ∑, Œ∂) = 4 * (1 - 2 * Œæ - Œ∑ - Œ∂)
@inline dN5_dŒ∑(Œæ, Œ∑, Œ∂) = -4 * Œæ
@inline dN5_dŒ∂(Œæ, Œ∑, Œ∂) = -4 * Œæ

@inline dN6_dŒæ(Œæ, Œ∑, Œ∂) = 4 * Œ∑
@inline dN6_dŒ∑(Œæ, Œ∑, Œ∂) = 4 * Œæ
@inline dN6_dŒ∂(Œæ, Œ∑, Œ∂) = 0.0

@inline dN7_dŒæ(Œæ, Œ∑, Œ∂) = -4 * Œ∑
@inline dN7_dŒ∑(Œæ, Œ∑, Œ∂) = 4 * (1 - Œæ - 2 * Œ∑ - Œ∂)
@inline dN7_dŒ∂(Œæ, Œ∑, Œ∂) = -4 * Œ∑

@inline dN8_dŒæ(Œæ, Œ∑, Œ∂) = -4 * Œ∂
@inline dN8_dŒ∑(Œæ, Œ∑, Œ∂) = -4 * Œ∂
@inline dN8_dŒ∂(Œæ, Œ∑, Œ∂) = 4 * (1 - Œæ - Œ∑ - 2 * Œ∂)

@inline dN9_dŒæ(Œæ, Œ∑, Œ∂) = 4 * Œ∂
@inline dN9_dŒ∑(Œæ, Œ∑, Œ∂) = 0.0
@inline dN9_dŒ∂(Œæ, Œ∑, Œ∂) = 4 * Œæ

@inline dN10_dŒæ(Œæ, Œ∑, Œ∂) = 0.0
@inline dN10_dŒ∑(Œæ, Œ∑, Œ∂) = 4 * Œ∂
@inline dN10_dŒ∂(Œæ, Œ∑, Œ∂) = 4 * Œ∑

# Evaluation function (returns tuple - zero allocation)
@inline function eval_basis_and_grad(xi::Vec{3})
    Œæ, Œ∑, Œ∂ = xi[1], xi[2], xi[3]

    N = (N1(Œæ, Œ∑, Œ∂), N2(Œæ, Œ∑, Œ∂), N3(Œæ, Œ∑, Œ∂), N4(Œæ, Œ∑, Œ∂), N5(Œæ, Œ∑, Œ∂),
        N6(Œæ, Œ∑, Œ∂), N7(Œæ, Œ∑, Œ∂), N8(Œæ, Œ∑, Œ∂), N9(Œæ, Œ∑, Œ∂), N10(Œæ, Œ∑, Œ∂))

    dN = (Vec(dN1_dŒæ(Œæ, Œ∑, Œ∂), dN1_dŒ∑(Œæ, Œ∑, Œ∂), dN1_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN2_dŒæ(Œæ, Œ∑, Œ∂), dN2_dŒ∑(Œæ, Œ∑, Œ∂), dN2_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN3_dŒæ(Œæ, Œ∑, Œ∂), dN3_dŒ∑(Œæ, Œ∑, Œ∂), dN3_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN4_dŒæ(Œæ, Œ∑, Œ∂), dN4_dŒ∑(Œæ, Œ∑, Œ∂), dN4_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN5_dŒæ(Œæ, Œ∑, Œ∂), dN5_dŒ∑(Œæ, Œ∑, Œ∂), dN5_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN6_dŒæ(Œæ, Œ∑, Œ∂), dN6_dŒ∑(Œæ, Œ∑, Œ∂), dN6_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN7_dŒæ(Œæ, Œ∑, Œ∂), dN7_dŒ∑(Œæ, Œ∑, Œ∂), dN7_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN8_dŒæ(Œæ, Œ∑, Œ∂), dN8_dŒ∑(Œæ, Œ∑, Œ∂), dN8_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN9_dŒæ(Œæ, Œ∑, Œ∂), dN9_dŒ∑(Œæ, Œ∑, Œ∂), dN9_dŒ∂(Œæ, Œ∑, Œ∂)),
        Vec(dN10_dŒæ(Œæ, Œ∑, Œ∂), dN10_dŒ∑(Œæ, Œ∑, Œ∂), dN10_dŒ∂(Œæ, Œ∑, Œ∂)))

    return N, dN
end
end

# ============================================================================
# ============================================================================
# METHOD 2: AD (Tensors.jl gradient)
# ============================================================================

module ADTet10
using Tensors

# Just shape functions (no manual derivatives!)
@inline N1(xi) = (1 - xi[1] - xi[2] - xi[3]) * (2 * (1 - xi[1] - xi[2] - xi[3]) - 1)
@inline N2(xi) = xi[1] * (2 * xi[1] - 1)
@inline N3(xi) = xi[2] * (2 * xi[2] - 1)
@inline N4(xi) = xi[3] * (2 * xi[3] - 1)
@inline N5(xi) = 4 * xi[1] * (1 - xi[1] - xi[2] - xi[3])
@inline N6(xi) = 4 * xi[1] * xi[2]
@inline N7(xi) = 4 * xi[2] * (1 - xi[1] - xi[2] - xi[3])
@inline N8(xi) = 4 * xi[3] * (1 - xi[1] - xi[2] - xi[3])
@inline N9(xi) = 4 * xi[1] * xi[3]
@inline N10(xi) = 4 * xi[2] * xi[3]

const shape_fns = (N1, N2, N3, N4, N5, N6, N7, N8, N9, N10)

@inline function eval_basis_and_grad(xi::Vec{3})
    # Evaluate basis functions
    N = ntuple(i -> shape_fns[i](xi), 10)

    # Compute gradients with Tensors.jl gradient()
    dN = ntuple(i -> gradient(shape_fns[i], xi), 10)

    return N, dN
end
end

# ============================================================================
# BENCHMARKING
# ============================================================================

println("Setting up benchmark...")
println()

# Test point (typical integration point)
const Œæ_test = Vec(0.25, 0.25, 0.25)

# Verification: Both methods should give same results
println("Verifying correctness...")
N_manual, dN_manual = ManualTet10.eval_basis_and_grad(Œæ_test)
N_ad, dN_ad = ADTet10.eval_basis_and_grad(Œæ_test)

println("  Manual basis: ", N_manual)
println("  AD basis:     ", N_ad)
println()

# Check agreement
rtol = 1e-10
if !all(isapprox.(N_manual, N_ad, rtol=rtol))
    @warn "Manual and AD basis functions disagree!"
end

# Check derivatives
for i in 1:10
    if !isapprox(dN_manual[i], dN_ad[i], rtol=rtol)
        @warn "Manual and AD derivative $i disagree!" dN_manual[i] dN_ad[i]
    end
end

println("‚úì Both methods agree (within tolerance)")
println()

# ============================================================================
# RUN BENCHMARKS
# ============================================================================

println("Running benchmarks (this may take a minute)...")
println()

# Warm-up
for _ in 1:1000
    ManualTet10.eval_basis_and_grad(Œæ_test)
    ADTet10.eval_basis_and_grad(Œæ_test)
end

# Benchmark each method
b_manual = @benchmark ManualTet10.eval_basis_and_grad($Œæ_test)
b_ad = @benchmark ADTet10.eval_basis_and_grad($Œæ_test)

# ============================================================================
# RESULTS
# ============================================================================

# ============================================================================
# RESULTS
# ============================================================================

println("="^70)
println("RESULTS")
println("="^70)
println()

# Extract median times
t_manual = median(b_manual.times)
t_ad = median(b_ad.times)

# Extract allocations
alloc_manual = b_manual.allocs
alloc_ad = b_ad.allocs

# Calculate relative speed
rel_ad = t_ad / t_manual

println("Method          | Time (ns) | Allocations | Relative Speed")
println("----------------|-----------|-------------|----------------")
@printf "Manual          | %9.1f | %11d | %.2f√ó (baseline)\n" t_manual alloc_manual 1.0
@printf "AD (Tensors.jl) | %9.1f | %11d | %.2f√ó\n" t_ad alloc_ad rel_ad
println()

# Detailed stats
println("Detailed Statistics:")
println()
println("Manual (Hand-Calculated):")
display(b_manual)
println()
println()
println("AD (Tensors.jl gradient):")
display(b_ad)
println()
println()

# ============================================================================
# ANALYSIS
# ============================================================================

println("="^70)
println("ANALYSIS")
println("="^70)
println()

if rel_ad < 2.0
    println("üéâ RECOMMENDATION: Use AD everywhere!")
    println()
    println("Tensors.jl AD is within 2√ó of manual, providing:")
    println("  ‚úì Zero maintenance burden")
    println("  ‚úì No manual derivative errors")
    println("  ‚úì Easy to add new elements")
    println("  ‚úì Supports any basis type")
    println()
    println("Small performance cost is acceptable for these benefits.")
elseif rel_ad < 5.0
    println("‚ö†Ô∏è  RECOMMENDATION: Hybrid approach")
    println()
    println("AD is 2-5√ó slower than manual. Consider:")
    println("  ‚Ä¢ Common elements (Tet10, Hex8, Quad4): Manual")
    println("  ‚Ä¢ Rare elements: AD-generated")
    println("  ‚Ä¢ Research/prototype elements: Always AD")
    println()
    println("This balances performance and maintainability.")
else
    println("‚ùå RECOMMENDATION: Manual derivatives (with symbolic generation)")
    println()
    println("AD is >5√ó slower than manual. For performance-critical code:")
    println("  ‚Ä¢ Generate derivatives with SymPy/Symbolics.jl")
    println("  ‚Ä¢ Unit test against AD to verify correctness")
    println("  ‚Ä¢ Accept the maintenance burden")
    println()
    println("Consider AD only for prototyping.")
end

println()
println("Memory analysis:")
if alloc_manual == 0 && alloc_ad == 0
    println("  ‚úì Both methods achieve zero allocations (excellent!)")
elseif alloc_manual == 0 && alloc_ad > 0
    println("  ‚ö†Ô∏è  AD allocates (", alloc_ad, " allocs)")
    println("     This will hurt performance in tight loops.")
else
    println("  ‚ö†Ô∏è  Unexpected allocation pattern - investigate!")
end

println()
println("="^70)
println("Benchmark complete! Results saved to console.")
println("="^70)
